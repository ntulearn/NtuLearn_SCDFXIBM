{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>emergency</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>body_len</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>['our', 'deeds', 'are', 'the', 'reason', 'of',...</td>\n",
       "      <td>['deeds', 'reason', 'earthquake', 'may', 'alla...</td>\n",
       "      <td>['deed', 'reason', 'earthquake', 'may', 'allah...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>['forest', 'fire', 'near', 'la', 'ronge', 'sas...</td>\n",
       "      <td>['forest', 'fire', 'near', 'la', 'ronge', 'sas...</td>\n",
       "      <td>['forest', 'fire', 'near', 'la', 'ronge', 'sas...</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>['all', 'residents', 'asked', 'to', 'shelter',...</td>\n",
       "      <td>['residents', 'asked', 'shelter', 'place', 'no...</td>\n",
       "      <td>['resident', 'asked', 'shelter', 'place', 'not...</td>\n",
       "      <td>112</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>['13000', 'people', 'receive', 'wildfires', 'e...</td>\n",
       "      <td>['13000', 'people', 'receive', 'wildfires', 'e...</td>\n",
       "      <td>['13000', 'people', 'receive', 'wildfire', 'ev...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>['just', 'got', 'sent', 'this', 'photo', 'from...</td>\n",
       "      <td>['got', 'sent', 'photo', 'ruby', 'alaska', 'sm...</td>\n",
       "      <td>['got', 'sent', 'photo', 'ruby', 'alaska', 'sm...</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 keyword location  \\\n",
       "0           0     NaN      NaN   \n",
       "1           1     NaN      NaN   \n",
       "2           2     NaN      NaN   \n",
       "3           3     NaN      NaN   \n",
       "4           4     NaN      NaN   \n",
       "\n",
       "                                                text  emergency  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...          1   \n",
       "1             Forest fire near La Ronge Sask. Canada          1   \n",
       "2  All residents asked to 'shelter in place' are ...          1   \n",
       "3  13,000 people receive #wildfires evacuation or...          1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...          1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1              Forest fire near La Ronge Sask Canada   \n",
       "2  All residents asked to shelter in place are be...   \n",
       "3  13000 people receive wildfires evacuation orde...   \n",
       "4  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  ['our', 'deeds', 'are', 'the', 'reason', 'of',...   \n",
       "1  ['forest', 'fire', 'near', 'la', 'ronge', 'sas...   \n",
       "2  ['all', 'residents', 'asked', 'to', 'shelter',...   \n",
       "3  ['13000', 'people', 'receive', 'wildfires', 'e...   \n",
       "4  ['just', 'got', 'sent', 'this', 'photo', 'from...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  ['deeds', 'reason', 'earthquake', 'may', 'alla...   \n",
       "1  ['forest', 'fire', 'near', 'la', 'ronge', 'sas...   \n",
       "2  ['residents', 'asked', 'shelter', 'place', 'no...   \n",
       "3  ['13000', 'people', 'receive', 'wildfires', 'e...   \n",
       "4  ['got', 'sent', 'photo', 'ruby', 'alaska', 'sm...   \n",
       "\n",
       "                                          lemmatized  body_len  sentiment  \n",
       "0  ['deed', 'reason', 'earthquake', 'may', 'allah...        57     0.2732  \n",
       "1  ['forest', 'fire', 'near', 'la', 'ronge', 'sas...        32    -0.3400  \n",
       "2  ['resident', 'asked', 'shelter', 'place', 'not...       112    -0.2960  \n",
       "3  ['13000', 'people', 'receive', 'wildfire', 'ev...        57     0.0000  \n",
       "4  ['got', 'sent', 'photo', 'ruby', 'alaska', 'sm...        72     0.0000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"clean.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform vectorizing on data\n",
    "\n",
    "- make use of tf-idf to find prominent words within each sentence to be used for classification\n",
    "- turn individual words into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20953</th>\n",
       "      <th>20954</th>\n",
       "      <th>20955</th>\n",
       "      <th>20956</th>\n",
       "      <th>20957</th>\n",
       "      <th>20958</th>\n",
       "      <th>20959</th>\n",
       "      <th>20960</th>\n",
       "      <th>20961</th>\n",
       "      <th>20962</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20963 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   20953  20954  20955  20956  20957  20958  20959  20960  20961  20962  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 20963 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf-idf vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(data['lemmatized'])\n",
    "\n",
    "# splits into features: body length, vader score and each number represents a single word\n",
    "X_tfidf_feat = pd.concat([pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_tfidf_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training & validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data with X as features and y as the label\n",
    "X_features = X_tfidf_feat\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['emergency'], test_size=0.3, shuffle=True,\n",
    "                                                   random_state=51, stratify=data.emergency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for printing results\n",
    "\n",
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.642 / Recall:0.608 / Accuracy:0.608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb_model = gnb.fit(X_train, y_train)\n",
    "y_pred = gnb_model.predict(X_test)\n",
    "precision, recall, fscore, support = score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "print('Precision:{} / Recall:{} / Accuracy:{}'.format(round(precision,3), round(recall,3),\n",
    "    round((y_pred==y_test).sum() / len(y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC()\n",
    "# parameters = {\n",
    "#     'kernel': ['linear', 'rbf', 'poly'],\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'degree': [1,2,3],\n",
    "#     'probability': [True]\n",
    "# }\n",
    "\n",
    "# cv = GridSearchCV(svc, parameters, cv=5)\n",
    "# cv.fit(X_train, y_train)\n",
    "\n",
    "# print_results(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss = 'log')\n",
    "parameters = {'alpha' : [10**(-x) for x in range(7)],\n",
    "             'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "             'l1_ratio' : [0.15, 0.25, 0.5, 0.75]}\n",
    "\n",
    "cv = GridSearchCV(lr, parameters, cv=5)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_estimator_\n",
    "\n",
    "Pkl_Filename = \"LR_Model.pkl\"  \n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(cv.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10,50,100,150,300],\n",
    "        'max_depth': [10,20,30,40,50,None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rf = RandomForestClassifier(n_estimators=50, max_depth=40, n_jobs=-1)\n",
    "final_rf_model = final_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename = \"RF_Model.pkl\"  \n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(final_rf_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "param = {'n_estimators': [10,50,100,150,300],\n",
    "        'max_depth': [3,7,11,15],\n",
    "        'learning_rate': [0.01, 0.1, 1]}\n",
    "\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "final_gb_model = final_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename = \"GB_Model.pkl\"  \n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(final_gb_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
